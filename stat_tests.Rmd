---
title: "Common statistical tests are linear models: a work through"
author: "Steve Doogue"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
description: This is a minimal example of using the bookdown package to write a book.
  The output format for this example is bookdown::gitbook.
documentclass: krantz
fontsize: 12pt
monofontoptions: "Scale=0.7"
link-citations: yes
bibliography:
- book.bib
- packages.bib
site: bookdown::bookdown_site
biblio-style: apalike
---

```{r, echo = FALSE}
library(knitr)
library(tidyverse)
library(kableExtra)
library(pixiedust)
# library(modelsummary)
options(knitr.kable.NA = '')
options(kableExtra.html.bsTable = T)
set.seed(40)
```

# Introduction

This is a reworking of the book [Common statistical tests are linear models (or: how to teach stats)](https://lindeloev.github.io/tests-as-linear/), written by Jonas Lindeløv. The book beautifully demonstrates how many common statistical tests (such as the t-test, ANOVA and chi-squared) are special cases of the linear model. The book also demonstrates that many non-parametric tests, which are needed when certain test assumptions do not hold, can be approximated by linear models using the _rank_ of values.

This approach offers a way to greatly simplify the teaching of introductory statistics, using the simple model of the form $y = a + b \cdot x$ which is familiar to most students. This approach brings coherence to a wide-range of statistical tests, which are usually taught to students as independent tools with a potentially-overwhelming array of names. The approach also helps to explain the intuition underlying statistical tests, drawing on the familiar concept of linear regressions, which emphasizes understanding over rote learning.

The purpose of creating this book is to solidify my understanding of this approach. I do this by reproducing the examples provided by Lindeløv; by expanding on areas where there were gaps in my knowledge; and by paraphrasing some of the explanations, using concepts and terms with which I am more familiar. The book may also be helpful to others who want to follow along with Lindeløv's book, but who require more background on some of the concepts being discussed.

Credit for this book should be attributed to Jonas Lindeløv, though I am of course responsible for any errors in my own interpretation.

Note that some of the data used in this book varies from that used by Lindeløv, and so some of the test results differ. However, the concepts being discussed are exactly the same.

_Steve Doogue_

<!--chapter:end:index.Rmd-->

# The data {#data}

_Note that some of the data used in this book varies from those used by Lindeløv. Some of the test results will therefore differ._

## Sample values {#samplevalues}
Most of the examples in the book are based on three imaginary samples (`x`, `y` and `y2`). Each is normally distributed and made up of 50 observations.

We start by creating a function that will allow us to produce samples of a given size (`N`) with a specified mean (`mu`) and standard deviation (`sd`):

```{r, message = FALSE}
rnorm_fixed <- function(n, mu = 0, sd = 1) {
  as.numeric(scale(rnorm(n)) * sd + mu)
}
```

Now we can create our three samples:

```{r}
# Create the samples (use the same order as original book - y, x, then y2)
y <- rnorm_fixed(50, mu = 0.3, sd = 2)
x <- rnorm_fixed(50, mu = 0, sd = 1)
y2 <- rnorm_fixed(50, mu = 0.5, sd = 1.5)
```

Let's also combine these samples into 'wide' and 'long' data frames. This doesn't change any of the values, it just rearranges the data into different layouts which can sometimes be easier to work with (e.g. when producing plots):

```{r, warning = FALSE}
# The wide layout is a dataframe with three columns, one for each of x, y and y2
mydata_wide <- tibble(x = x, y = y, y2 = y2)

# the long layout has two columns: one listing the group to which each
# observation elongs (x, y or y2), and another column with the corresponding value
mydata_long <- mydata_wide %>%
  gather(group, value, x:y2) 
```

Here's what our three samples look like when plotted. Note the different mean (0.0, 0.3, and 0.5) and the different 'spread' of values for each group (reflecting their different standard deviations).

```{r, echo = FALSE, warning = FALSE, fig.cap = 'Our sample data',fig.align = 'center',fig.width= 5,fig.height = 4}
ggplot(mydata_long, aes(x = group, y = value, color = group)) +
  geom_jitter(width = 0.1, alpha = .75, size  = 2) +
  stat_summary(fun = mean,
                  fun.min = mean,
                  fun.max = mean,
                  geom = "crossbar",
                  width = 0.5,
                  size = 0.2,
                  color = "black") +
  stat_summary(fun = mean,
                  colour = "black",
                  geom = "text",
                  show.legend = FALSE,
                  vjust = -0.7,
                  aes(label = round(..y.., 2)))
```

## 'Signed rank' values {#ranktrans}

Most common tests demonstrated in the book use the _actual_ values from the samples above. However, some tests (i.e. non-parametric tests) also use their _rank-transformed_ values. In some cases, the _signed rank_ of the values is used.

What is meant by signed ranks? The signed rank is found by:
1. taking the _absolute_ value of each observation in the original sample; that is, expressing all the values as a positive number;
2. finding the rank of each of these absolute values, where the smallest absolute value has a rank of 1; and
3. giving each of these ranks the same sign (+ or -) as the original value.

For example, the numbers -2, 3, 7, -25, -30, 31 would have the signed ranks of -1, 2, 3, -4, -5, 6.

We create a function, `signed_rank()`, that we can use later to convert our actual values into signed ranks:

```{r}
# Takes any list of values, x, and calculates their signed rank
signed_rank <- function(x) sign(x) * rank(abs(x))
```

<!--chapter:end:01-data.Rmd-->

# The linear model  {#linearmodel}

## Overview of the linear model

The premise of this book is that many common statistical tests are really just special cases of the __linear model__.

This section provides an overview of the linear model. The description will be somewhat informal as it aims to provide an intuitive explanation rather than a rigorous technical description.

The linear model, or linear regression model, estimates the relationship between one continuous variable and one or more other variables. It is assumed that the relationship can be described as a straight line (hence the term 'linear').

For example, say we are looking at a variable $y$ and we want to know its relationship with a variable $x$. We assume that the relationship can be expressed as a mathematical relationship between $y$ (the dependent, or response variable) and $x$ (the explanatory variable):

::: {.center data-latex=""}

$y = \beta_0 + \beta_1 x$

:::

To illustrate what this equation is showing, imagine we have six observations of variables $x$ and $y$, which can be plotted as follows:

```{r, echo = FALSE, fig.cap = "Six observations of x and y", out.width = "80%",fig.align = "center"}
knitr::include_graphics("images/image01.png")
```

We assume that this relationship can be represented by a straight line. The line is composed of an intercept ($\beta_0$) and a slope ($\beta_1$). Each point on this line represents our _predicted_ value of $y$ for a given value of $x$.

```{r, echo = FALSE, fig.cap = "Intercept and slope of model", out.width = "80%",fig.align = "center"}
knitr::include_graphics("images/image02.png")
```

So how do we estimate the intercept and slope? In other words, how do we estimate what the values of $\beta_0$ and $\beta_1$ should be?

Linear regression estimates the line (i.e., slope and intercept) that minimizes the difference between our _predicted_ values of $y$ and the _actual_ values of $y$ that were observed in the original sample. These difference are referred to as "residuals".

More specifically, linear regression minimizes the sum of the _squared_ value of these residuals. So in the figure below, linear regression is used to estimate the line that would minimize the combined area of the purple squares. For this reason, the method is sometimes referred to as an "ordinary least squares" (OLS) regression.

```{r, echo = FALSE, fig.cap = "Line of best fit minimizes the sum of squared residuals", out.width = "60%",fig.align = "center"}
knitr::include_graphics("images/image03.png")
```

In the example above, we considered a dependent variable ($y$) that was being "explained" by one other variable, or predictor ($x$). But this can be expanded to include multiple predictors, for example with the model:

::: {.center data-latex=""}

$y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 ... + \beta_i x_i$

:::

This is referred to as _multiple regression_. In this case we still have only one intercept ($\beta_0$) but many slopes that need to be estimated ($\beta_1$, $\beta_2$ and so on). The same method can be used to estimate these parameters as was illustrated above, i.e. an OLS regression which minimizes the sum of squared residuals. This cannot be illustrated in a two-dimensional diagram, as was the case when a single predictor was used, but the exact same concept applies.

So that's the linear model. The key premise of Lindeløv's book many statistical tests are just special cases of this:

> Everything below, from one-sample t-test to two-way ANOVA are just special cases of this system. Nothing more, nothing less.

## Estimating linear models in R

To estimate linear models in R we use the `lm()` function.

For example, say we want to estimate the relationship between our sampled data for `x` and `y`. We will apply the following model: $y = \beta_0 + \beta_1 x$

In R, this can be written as follows:
```{r, eval = FALSE}
# Represents y = beta0 + beta1 * x
lm(y ~ 1 + x)
```

In the original book, the specified model is accompanied by the null hypothesis, $H_0: \beta_1 = 0$. This is equivalent to saying that there is no relationship between `x` and `y`. The output from our linear model tells us whether there could be grounds for rejecting this null hypothesis of "no relationship", in favor of the alternative hypothesis that there _is_ a relationship.

A key output of interest, or test statistic, is the p-value.

* A small p-value (conventionally ≤ 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis of no relationship.^[The p-value is the probability of getting a value as extreme or more extreme than the one observed in your sample, under the assumption that the null hypothesis is true.]
* A larger p-value (> 0.05) indicates weak evidence against the null hypothesis, so you fail to reject the null hypothesis of no relationship.

Let's see the output of the linear model for our sample data:
```{r}
lm(y ~ 1 + x) %>% summary() %>% #print(digits = 5)
  # kable()
  sprinkle(col = 2:4, round = 3) %>% 
  sprinkle(col = 5, fn = quote(pvalString(value))) %>% 
  sprinkle_colnames(term = "Term", 
                    estimate = "Estimate", 
                     std.error = "SE",
                     statistic = "T-statistic", 
                     p.value = "P-value") %>%
  kbl() %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE) 
```

As seen in the output above, $\beta_1$ (the coefficient on `x`) has a p-value of 0.1053. This means we would fail to reject the null hypothesis that there was no relationship between `x` and `y`.

## Assumptions

The inferences drawn from the linear model, as described above, are only valid if a number of assumptions hold. The following diagram, found in the book [Broadening Your Statistical Horizons](https://bookdown.org/roback/bookdown-bysh/), can be used to explain these assumptions:

```{r, echo = FALSE, fig.cap = "Ordinary least squares assumptions", out.width = "80%",fig.align = "center"}
knitr::include_graphics("images/ols_assumptions.png")
```

The four main assumptions can be remembered using the acronym 'LINE':

__L:__ there is a __linear__ or straightline relationship between the mean response (Y) and the explanatory variable (X). In the figure above, the mean value for Y at each level of X falls on the blue regression line.

__I:__ the errors are __independent__ — there’s no connection between how far any two points lie from the regression line. This is usually more of an issue in time series data (i.e. where data is collected from the same entity over time, e.g. a stock price) than cross-sectional data (where data is collected from many entities a single point in time, e.g. exam marks for a group of students).

__N:__ the dependent variable is __normally__ distributed at each level of X. At each level of X, the values for Y follow a normal or 'bell-shaped' distribution as shown in red in the figure above.

__E:__ there is __equal variance__ (or 'homoscedasticity') - the variance or 'spread' of the responses is equal for all levels of X. The spread in the Y’s for each level of X is the same, as shown above.

A more detailed explanation of these assumptions is provided here by [Laerd](https://statistics.laerd.com/spss-tutorials/linear-regression-using-spss-statistics.php).

<!--chapter:end:02-linear_model.Rmd-->

# Correlations

Correlation is a measure of the strength and direction of association that exists between two variables. Correlation coefficients ($r$) assume values in the range from −1 to +1, where ±1 indicates the strongest possible positive or negative correlation and 0 indicates no linear association between the variables.

## Pearson correlation

We start by looking at the Pearson correlation coefficient. Here we compare the built-in test (in R) for the Pearson correlation with the equivalent linear model.

The equivalent linear model is the basic regression of $y$ on $x$, specified as follows:

::: {.center data-latex=""}

$y = \beta_0 + \beta_1x  \qquad  H_0: \beta_1 = 0$

:::

The two tests are written using the following R code:

```{r, eval = FALSE}
# Pearson (built in test)
cor.test(y, x, method = "pearson")
# Linear model
lm(y ~ 1 + x) %>% summary() %>% print(digits = 8)
```

If you were to run the code above, you would see the following the key statistics found in the output of each test:

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~r, ~slope, ~t, ~"p-value",
    "cor.test (Pearson)", -0.2318, NA, -1.6507, 0.1053,
    "lm", NA, -0.4636, -1.6507, 0.1053
    ),
  caption = "Pearson vs linear model",
  booktabs = TRUE
)
```


The output shows that the correlation coefficient ($r$) has a p-value of 0.1053, which is exactly the same as the p-value for the slope of the linear model. In this case, we would not reject the null hypothesis that there was no correlation between the two variables (at the 0.05 level of significance).

The main difference is that the linear model returns the _slope_ of the relationship, $\beta_1$ (which in this case is -0.4636), rather than the correlation coefficient, $r$. The slope is usually much more interpretable and informative than the correlation coefficient.

__Additional note (optional):__

It may be useful to understand how the Pearson correlation coefficient ($r$) and the regression coefficient or slope ($\beta_1$) are related, which is by the following formula:

::: {.center data-latex=""}

$\beta_1 = r \cdot sd_y / sd_x$

:::

This shows that:

* When both `x` and `y` have the same standard deviations ($sd_x$ and $sd_y$) then the slope ($\beta_1$) will be equal to the correlation coefficient ($r$)
* The ratio of the slope to the correlation coefficient ($\beta_1 / r$) is equal to the ratio of the standard deviations ($sd_y / sd_x$). In this example, the standard deviation of `y` is exactly twice as large as `x`, which is why the slope has twice the magnitude of the correlation coefficient.
* The slope from the linear model will always have the same sign (+ or -) as the correlation coefficient (as standard deviations are always positive).



## Spearman correlation

There will be times when it is more appropriate to use the __Spearman rank correlation__ than the Pearson correlation. This could be the case when:

1.  The relationship between the variables is not linear, i.e. not a straight line;
2.  The data is not normally distributed;^[Technically the variables should have _bivariate_ normality, which means they are normally distributed when added together, but this is complex and so it is common just to assess whether the variables are individually normal (explained here on the [Laerd](https://statistics.laerd.com/spss-tutorials/pearsons-product-moment-correlation-using-spss-statistics.php) website). If bi-variate normality does not hold then you will still get a fair estimate of $r$, but the inferential tests (t-statistics and p-values) could be misleading (explained [here](https://www.researchgate.net/post/Why_should_data_be_normally_distributed_and_continuous_in_order_to_apply_Pearson_correlation)).]
3.  The data has large outliers; or
4.  When you are working with ordinal rather than continuous data.^[see Chapter \@ref(appendixtypes) for a description of the different types of data.]

The Spearman correlation is a _non-parameteric_ test as it does not require that the parameters of the linear model hold true. For example, there does not need to be a linear relationship between the two variables, and the data does not need to be normally distributed.

The Spearman rank correlation is the same as a Pearson correlation but using the _rank_ of the values in our samples. This is an approximation only, which Lindeløv shows is approximate when the sample size is greater than 10 and almost perfect when the sample is greater than 20.

This is also the same as the linear model using rank-transformed values of $x$ and $y$:

::: {.center data-latex=""}

$rank(y) = \beta_0 + \beta_1 \cdot rank(x)  \qquad  H_0: \beta_1 = 0$

:::

For a comparison of the Spearman test, the Pearson test (using ranks) and the linear model (also using ranks) we run the following code:

```{r, eval = FALSE}
# Spearman
cor.test(y, x, method = "spearman")

# Pearson using ranks
cor.test(rank(y), rank(x), method = "pearson")

# Linear model using rank
lm <- lm(rank(y) ~ 1 + rank(x))
  lm %>% summary() %>% print(digits = 5) # show summary ouput
```

The output of this code is as follows:

```{r, echo = FALSE}
# knitr::kable(
#   tribble(
#     ~Test, ~"correlation", ~slope, ~p.value,
#     "cor.test (Spearman)", -0.2266, NA, 0.1135,
#     "cor.test (Pearson with ranks)", -0.2266, NA, 0.1135,
#     "lm (with ranks)", NA, -0.2266, 0.1135
#     ),
#   caption = "Spearman, Pearson (ranks) and linear model (ranks)",
#   booktabs = TRUE,
# )

tribble(
    ~Test, ~"Correlation", ~Slope, ~p.value,
    # "cor.test (Spearman)", -0.2266, NA, 0.1135,
    "Spearman", -0.2266, NA, 0.1135,
    # "cor.test (Pearson with ranks)", -0.2266, NA, 0.1135,
    "Pearson with Ranks", -0.2266, NA, 0.1135,
    "lm (with ranks)", NA, -0.2266, 0.1135
    ) %>%
  kbl(caption = "Spearman, Pearson (ranks) and linear model (ranks)") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), full_width = FALSE) %>%
  pack_rows("cor.test", 1, 2  , label_row_css = "background-color: #D3D3D3; color: #fff;")
```

This shows that the results are all the same (or at least very close approximations). When ranks are used, the slope of the linear model ($\beta_1$) has the same value as the correlation coefficient ($r$). Note that the slope from the linear model has an intuitive interpretation, which is the number of ranks $y$ changes for each change in rank of $x$.

Given the similarity of these tests, Lindeløv notes that:

> One interesting implication is that many “non-parametric tests” are about as parametric as their parametric counterparts with means, standard deviations, homogeneity of variance, etc. - just on rank-transformed data.

Finally, the figure below (reproduced from the original book) illustrates how the Pearson and Spearman correlations are equivalent to linear models, with the latter being based on rank-transformed values.

```{r, echo = FALSE, fig.cap = "Pearson and Spearman correlations as linear models", out.width = "80%",fig.align = "center"}
knitr::include_graphics("images/correlations.png")
```

<!--chapter:end:03-correlations.Rmd-->

# One mean

This set of tests deals with a single mean. They tell us whether, based on our sample, we have reason to believe that the mean of the underlying population differs from an some specific level (the 'null hypothesis').

In some cases we might have two samples with _paired_ observations (e.g. an athlete's average running speed before and after a new training technique is used). In this case we take the _difference_ between the paired observations (the change in each athlete's running speed) and treat this as a single measurement. We then assess whether, based on the mean of these differences in our sample, we have reason to believe that the 'true' mean differs from a specified level (e.g. zero, representing
no change in running speed).

The _one sample_ and _paired sample_ cases are addressed in turn, below.

## One sample

### One-sample t-test

A one-sample t-test (or one-sample Student's t-test) is used to determine whether a sample could have come from a population with a specified mean. This population mean is not always known and may only be theoretical / hypothesized.

For example, say we take a sample of pupils who have been taught using a new teaching method. These pupils receive an average score of 70% in an end-of-year test, while the average score for all pupils nationally is 60%. We want to know how often we are likely to see a sample average of 70% (or more extreme) if the 'true' average for pupils receiving the new teaching method was 60%, i.e. if there was no difference from the national average.

__Student's t-test:__

In R, we can run a one-sample Student's t-test using the built-in `t.test()`. The documentation for this test can be found [here](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/t.test).


__Equivalent linear model:__

The one-sample t-test is the same as a linear model containing a dependent variable, $y$, and a constant only. That is, a model with the form:

::: {.center data-latex=""}

$y = \beta_0  \qquad  H_0: \beta_0 = 0$

:::

This is the same as the equation $y = \beta_0 + \beta_1 \cdot x$ that was shown earlier (Chapter \@ref(linearmodel)), but having dropped the last term. This is shown graphically below. Note that this is the same as a linear regression in which all values of $x$ are treated as zero:

```{r, echo = FALSE, fig.cap = "Linear model equivalent to one-sample t-test", out.width = "40%", fig.align = "center"}
knitr::include_graphics("images/image04.png")
```


The estimated intercept ($\beta_0$) is simply the average of all the values in our sample. The linear regression returns the t-statistic and p-value based on the null hypothesis that the true average is equal to zero (i.e. $\beta_0 = 0$).

__Comparison:__

Here is how you would run the t-test and equivalent linear model in R:

```{r, eval = FALSE}
# t-test (built-in test)
t.test(y, mu = 0, alternative = "two.sided")

# Linear model
lm <- lm(y ~ 1)
  lm %>% summary() %>% print(digits = 5) # show summary output
  confint(lm) # show confidence intervals
```

The output of this code, including the 95 percent confidence intervals, is shown in the table below. The outputs are exactly the same!

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"mean / intercept", ~t, ~p.value, ~conf.low, ~conf.high,
    "t.test", 0.3, 1.0607, 0.2940, -0.2684, 0.8684,
    "lm", 0.3, 1.0607, 0.2940, -0.2684, 0.8684
    ),
  caption = "One-sample t-test and linear model",
  booktabs = TRUE
)
```

Based on the p-value above, despite having a sample average of 0.3 we would not reject the null hypothesis that the true or population average was zero (at the 0.05 level of significance).

##### Testing a null hypothesis other than zero: {-}
By default, the t-test has a null hypothesis is that the sample comes from a population with a mean of _zero_. This was written as $H_0: \beta_0 = 0$ above. What if we want to use a different null hypothesis? For example, to how likely it is that our sample `y` came from a population with a mean of 0.1?

This can be done by specifying the null hypothesis in the built-in t-test (in this case using the argument ` mu = 0.1`) or, in the linear model, by first subtracting this amount from the dependent variable (using `y - 0.1` as the dependent variable).

The code is as follows:

```{r, eval = FALSE}
# t-test (built-in test)
t.test(y, mu = 0.1, alternative = "two.sided")

# Linear model
lm((y - 0.1) ~ 1) %>%
  summary() %>%
  print(digits = 5) # show summary output
```

The two tests above give identical t statistics and p-values.

### Wilcoxen signed-rank

What if we cannot assume that our population is normally distributed? Here we need to use a non-parametric version of the t-test.

__Wilcoxen signed-rank test:__

In this case we could use the Wilcoxon signed-rank test, which is the non-parametric version of the t-test. Specifically, we use the one-sample Wilcoxon signed-rank test. This is used to determine whether the _median_ of the sample is equal to a theoretical value, such as zero, under the [assumption](http://www.sthda.com/english/wiki/one-sample-wilcoxon-signed-rank-test-in-r) that the data is symmetrically distributed.

The Wilcoxon signed-rank test is very similar to the linear model described above, but using the _signed ranks_ of $y$ instead of $y$ itself. The concept of signed ranks was explained in section \@ref(ranktrans).

It is implemented in R using the [wilcox.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/wilcox.test), as shown below.

__Equivalent linear model:__

In this case the equivalent linear model is:

::: {.center data-latex=""}

$signed\_rank(y) = \beta_0  \qquad  H_0: \beta_0 = 0$

:::

[Lindeløv shows](https://lindeloev.github.io/tests-as-linear/simulations/simulate_wilcoxon.html) that the linear model will be a good approximation of the Wilcoxon signed-rank test when the sample size is larger than 14 and almost perfect when the sample size is larger than 50.

__Comparison:__

The code below is for the [wilcox.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/wilcox.test) in R and the equivalent linear model using signed ranks:

```{r, eval = FALSE}
# Wilcox test (built-in)
wilcox.test(y)

# Equivalent linear model
lm <- lm(signed_rank(y) ~ 1)
  lm %>% summary() %>% print(digits = 8) # show summary output
```

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"mean / intercept", ~p.value,
    "wilcox.test", NA, 0.3693,
    "lm (signed ranks)", 3.74, 0.3721
    ),
  caption = "Wilcox signed rank test and linear model",
  booktabs = TRUE
)
```

The two tests give similar (though not quite identical) p-values.

## Paired samples

### Paired-sample t-test

A __paired-sample t-test__ (sometime called a dependent-sample t-test) is used to compare two population means where you have two samples, in which observations in one sample can be paired with observations in the other sample.

Common applications of the paired-sample t-test include controlled studies or repeated-measures designs. Examples of when you might use this test include:

* Before and after observations on the same subjects (e.g. students’ test
results before and after taking a course).
* A comparison of two different treatments, where where the treatments are applied to the same subjects (e.g. athletes' ability to lift weights following two different warm-up routines).
* A comparison of two different measurements, where where the measurements are applied to the same subjects (e.g. blood pressure measured using two types of machines).

__Paired-sample t-test / dependent-sample t-test:__

The paired-sample t-test determines whether the average difference between two sets of observations is zero. To run this in R, we use the same Student's t-test as above, but now include both variables as arguments and specify that we have paired observations (using the argument `paired = TRUE`).

__Equivalent linear model:__

The equivalent linear model is the _difference_ between the two observations regressed against a constant:

::: {.center data-latex=""}

$y_2 - y_1 = \beta_0 \qquad  H_0: \beta_0 = 0$

:::

__Comparison:__

We can compare the t-test and linear model using our sample data (section \@ref(samplevalues)) as an example. Recall that `y` had a mean of 0.3, and `y2` had a mean of 0.5.

The code and the outputs are shown below. Again, the linear model gives exactly the same results as the t-test!

```{r, eval = FALSE}
# t-test (built-in test)
t.test(y2, y, paired = TRUE, mu = 0, alternative = "two.sided")

# Equivalent linear model
lm <- lm(y2 - y ~ 1)
  lm %>% summary() %>% print(digits = 8)
  confint(lm)
```

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"mean of diff / intercept", ~t, ~p.value, ~conf.low, ~conf.high,
    "t.test",  0.2, 0.5264, 0.601, -0.5635, 0.9635,
    "lm", 0.2, 0.5264, 0.601, -0.5635, 0.9635
    ),
  caption = "Paired-sample t-test and linear model",
  booktabs = TRUE
)
```

This shows us that the difference in averages between `y` and `y2` is 0.2, as would be expected, but that this difference is not statistically significantly difference from zero (p = 0.601) at the 0.05 level of significance.

### Wilcoxen matched pairs

If the necessary assumptions do not hold for a paired-sample t-test - such as normally distributed data - we can use the non-parametric counterpart. This is the __Wilcoxen matched pairs__ test. The only difference from the Wilcoxon signed-rank test is that it’s testing the signed ranks of the pairwise $y−x$ differences.

This is used to test the null hypothesis that the _median_ of the differences in our paired observations are zero. An assumption is that the values of the pairwise differences are symmetrically distributed, as explained by [Laerd](https://statistics.laerd.com/spss-tutorials/wilcoxon-signed-rank-test-using-spss-statistics.php).

For this we use the built-in [wilcox.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/wilcox.test) in R, including both variables and specifying that we have paired observations (the argument `paired = TRUE`).

The equivalent linear model is exactly the same above but using _signed rank_ of the difference between observations, i.e.:

::: {.center data-latex=""}

$signed\_rank(y_2 - y_1) = \beta_0 \qquad  H_0: \beta_0 = 0$

:::

A comparison of the outputs is shown below. The p-values are almost identical. The t-test has also been included, this time using signed ranks, to show that this is the same as the linear model.

```{r, eval = FALSE}
#  Wilcox test (built-in test)
wilcox.test(y2, y, paired = TRUE, mu = 0, alternative = "two.sided")

# Equivalent linear model
lm <- lm(signed_rank(y2 - y) ~ 1)
  lm %>% summary() %>% print(digits = 8)

# t-test using signed ranks (built-in test)
t.test(signed_rank(y2 - y), mu = 0, alternative = "two.sided")
```

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"mean / intercept", ~p.value,
    "wilcox.test", NA, 0.8243,
    "lm (signed ranks)", 0.94, 0.8232,
    "t.test (signed ranks)",  0.94, 0.8232
    ),
  caption = "Wilcox test and linear model, with paired samples",
  booktabs = TRUE
)
```

Based on the results above, we would not reject the null hypothesis that the _median_ change between `y` and `y2` was zero (p = 0.8232), at the 0.05 level of significance.

<!--chapter:end:04-one_mean.Rmd-->

# Two means (independent samples)

These tests compare the means of two independent or unrelated groups, in order to determine whether there is statistical evidence that the population means are significantly different. Examples include:

* Do first year graduate salaries differ based on gender? and
* Is there is a difference in test anxiety based on educational level (undergraduate vs postgraduate)?

A key question is whether or not the __variances__ of the two samples being tested are equal.^[This can be tested with the Levene’s Test for Equality of Variances.] In general terms:

* If the samples have _equal_ variance then an __independent-sample t-test__ (Student's t-test) could be used.
* If the samples have _unequal_ variance then the __Welch's t-test__ can be used, as this does not assume identical variances.
* If the samples are _not normally distibuted_, or if the other requirements of the above tests are not met, then the non-parametric __Mann-Whitney U__ test could be used.

This is somewhat simplistic, and the choice of tests is not always clear (see the discussion [here](https://stats.stackexchange.com/questions/121852/how-to-choose-between-t-test-or-non-parametric-test-e-g-wilcoxon-in-small-sampl) on StackExchange for example). The following sections goes through each of these tests in turn, and describes how they can be approximated by an equivalent linear model.

## Independent t-test

The independent t-test can be used to compare the means of two unrelated groups. Assumptions include:

* Independence of observations (independent samples / groups);
* Normal distribution (approximately) of the dependent variable for each group, though this might not be an issue for large samples;
* No outliers; and
* Homogeneity of variances; i.e. variances are approximately equal across the two groups.

__Student's t-test:__

In R, we can assesses the difference between two groups using the [t.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/t.test). This has the default assumptions that the two samples are _not_ paired, and that their variances are _not_ equal.

__Equivalent linear model:__

The equivalent linear model uses a dummy variable to represent the two groups (see the explanation of dummy variables explained by Lindeløv in section 5.1.3 of the [original book](https://lindeloev.github.io/tests-as-linear/#51_independent_t-test_and_mann-whitney_u)).

The linear model takes the form:

::: {.center data-latex=""}

$y = \beta_0 + \beta_1 \cdot x_i \qquad  H_0: \beta_0 = 0$

:::

where $x_i$ is a dummy variable taking the value of 0 or 1, indicating whether observation $i$ is from the reference group (0) or the other group (1). For example, $y$ could be a measure of income and $x_i$ could be a dummy variable taking the value of 0 for women and 1 for men.

The use of dummy variables is illustrated below. The intercept ($\beta_0$) is the sample average for observations in our reference group (in this example, women) when $x = 0$. The slope ($\beta_1$) is the difference in the averages of the two groups. In this example, when $x = 1$ (indicating men), the average salary is equal to $\beta_0 + \beta_1$. Therefore, $\beta_1$ is the difference between the two groups, and we want to assess whether this difference is significantly different from zero.

```{r dummy2, echo = FALSE, fig.cap = "Linear model equivalent to independent-sample t-test", out.width = "65%", fig.align = "center"}
knitr::include_graphics("images/image_dummy1.png")
```


__Comparison:__

Let's say we want to compare the means of two groups in our example dataset, `y` and `y2`.

We start by creating a dummy variable to represent the two groups. The new variable is called `group_y2` and takes a value of 0 for observations in group `y` and a value of 1 for observations in group `y2`. This is done using the following code:


```{r, echo = FALSE}
mydata_dummy <- mydata_long %>%
  # Filter out "x" group which is not used in this example
  filter(group != "x") %>%
  # Create a dummy variable that takes 0 for the group "y" and 1 for group "y2"
  mutate(group_y2 = if_else(group == "y2", 1, 0))

```

Here's a sample of six rows from our new data set, which now includes the dummy variable:

```{r, echo = FALSE}
knitr::kable(
  mydata_dummy[c(1:3, 51:53), ],
  caption = "Some randomly selected rows from our data set",
  booktabs = TRUE)

```

Now we've organized our data, we can compare the results of the built-in independent t-test with the linear model:

```{r, eval = FALSE}
# independent t-test (built-in test)
t.test(y2, y, var.equal = TRUE)
# default is mu = 0, i.e. the null hypothesis that the difference in means is 0

# Linear model
lm <- lm(value ~ 1 + group_y2, data = mydata_dummy)
  lm %>% summary() %>% print(digits = 8) # show summary output
  confint(lm) # show confidence intervals
```

The outputs from the independent t-test and the equivalent linear model are shown below. The t statistic, p-value and confidence intervals are identical. While the t-test reports the averages of the two samples, the linear model reports the average of the reference group (0.3 for `y`) and the _difference_ between the average of this reference group and the other group indicated by the dummy variable (a difference of +0.2).

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"mean.y", ~"mean.y2", ~"difference", ~t, ~p.value, ~conf.low, ~conf.high,
    "t-test", 0.3, 0.5, NA, 0.5657, 0.5729, -0.5016, 0.9016,
    "lm (with dummy)", 0.3, NA,  0.2, 0.5657, 0.5729, -0.5016, 0.9016
    ),
  caption = "Independent-sample t-test (equal variance) and linear model",
  booktabs = TRUE
)
```

## Welch's t-test

If the two samples have unequal variance then Welch's t-test could be used.

In fact, [there is an argument](http://daniellakens.blogspot.com/2015/01/always-use-welchs-t-test-instead-of.html) that we should use Welch’s t-test by default, rather than the independent Student’s t-test, because Welch's t-test performs better than the t-test whenever sample sizes and variances are unequal between groups, and gives the same result when sample sizes and variances are equal.

__Welch's t-test:__

The Welch t-test is identical to the independent-sample Student's t-test described above, except that it does not assume equal variance of the two samples. In R, we can use the built-in [t.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/t.test). When entering the code, we just need to specify that the sample variances are __not equal__ (though this is the default assumption anyway).

__Equivalent linear model:__

The equivalent linear model is based on generalized least squares (GLS). The GLS approach makes the assumption that there is a relationship between the variance of observations and one of the independent variables used in our regression (in this example, the gender dummy variable). In a GLS regression, less weight is given to the observations with a higher variance.

We don't know the true relationship between variance and our independent variable(s) in the underlying population, so this relationship is estimated from our sample. The estimate is based on the relationship between residuals (i.e. observed values minus predicted values, based on an OLS regression) and an independent variable. Once we have estimated this relationship, we then re-weight each observation in our sample by dividing each observation by the predicted variance. The final GLS regression is then run on these re-weighted observations. A more detailed explanation of the GLS approach (without using matrix algebra!) [can be found here](http://www.homepages.ucl.ac.uk/~uctpsc0/Teaching/GR03/Heter&Autocorr.pdf).

In this example, when applying the GLS model in R, we specify that there is a different variance for group `y` and group `y2`.

__Comparison:__

The Welch's t-test and the equivalent linear model are carried out as follows:

```{r, eval = FALSE}
options(digits = 10)
# t-test (built-in test)
t.test(y2, y, mu = 0, var.equal = FALSE, alternative = "two.sided")
# Note the assumption that variances are false

# Linear model (GLS)
lm <- nlme::gls(value ~ 1 + group_y2,
                weights = nlme::varIdent(form = ~1|group),
                method = "ML",
                data = mydata_dummy)
  lm %>% summary() %>% print(digits = 8) # show summary output
  confint(lm) # show confidence intervals
```

Here are the results, which are almost identical:

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"mean.y", ~"mean.y2", ~"difference", ~t, ~p.value, ~conf.low, ~conf.high,
    "t-test", 0.3, 0.5, NA, 0.5657, 0.5730, -0.5023, 0.9023,
    "lm (GLS)", 0.3, NA, 0.2, 0.5657,	0.5729, -0.4930, 0.8930
    ),
  caption = "Independent-sample t-test (unequal variance) and GLS model",
  booktabs = TRUE
)
```

Note that we get the same estimates for the means of $y$ and $y_2$ (of 0.3 and 0.5, respectively) from the t-test both with and without the assumption of equal variance. The unequal variance (heteroscedasticity) does not affect our estimates of the population means, but rather our assessment of whether or differences are statistically significant, in the form of t statistics, p-values and confidence intervals.

## Mann-Whitney U test

If our usual assumptions don't hold (e.g. normal distributions, or if we're working with ordinal data) we can use a non-parametric version of these tests instead. When comparing two independent samples, this would be the Mann-Whitney U test.

__Mann-Whitney U test:__

This tests the null hypothesis that it is equally likely that a randomly selected value from one sample will be less than or greater than a randomly selected value from a second sample. In other words, if the null hypothesis was rejected, you would infer that values from one population are more likely to be higher or lower than values from another population.

The Man-Whitney U test is a Wilcoxon test but with two samples. It can be run using R's built-in [wilcox.test](https://www.rdocumentation.org/packages/stats/versions/3.5.3/topics/wilcox.test), this time with the (default) assumption that the observations are not paired.

__Equivalent linear model:__

Another way to describe the Mann-Whitney U test is as a test of mean ranks. It first ranks all your values for the dependent variable (e.g. income) from high to low, with the smallest rank assigned the smallest value. It then computes the mean rank in each group (e.g. male or female), and then computes the probability than random shuffling of those values between two groups would end up with the mean ranks as far apart as, or further apart, than you observed (see for example [StackExchange](https://stats.stackexchange.com/questions/113334/mann-whitney-test-with-unequal-variances) and [Laerd](https://statistics.laerd.com/premium-sample/mwut/mann-whitney-test-in-spss-2.php)). If the resulting p-value was sufficiently small, you would infer that the difference in mean ranks between the samples was probably not due to chance, and that the values from one population were higher than those from another.

The equivalent linear model, with a close approximation, is a regression using the rank of the dependent variable:

::: {.center data-latex=""}

$rank(y_i) = \beta_0 + \beta_1 \cdot x_i \qquad  H_0: \beta_0 = 0$

:::

where again $x_i$ is a dummy variable indicating the group to which an observation belongs (in this example, 0 for women and 1 for men). Note that this uses the rank of the dependent variable, and not the signed rank as was the case with matched pairs.

__Comparison:__

Here is how you would run the Man-Whitney U test and equivalent linear model in R:

```{r, eval = FALSE}
# Wilcoxon / Mann-Whitney U test (built-in)
wilcox.test(y, y2, paired = FALSE)

# Equivalent linear model
lm <- lm(rank(value) ~ 1 + group_y2, data = mydata_dummy)
  lm %>% summary() %>% print(digits = 8) # show summary output
```

The results are shown below. Both tests have very similar p-values. On this basis, we would not reject the null hypothesis that a value randomly selected from group `y` was not more likely to be higher or lower than one sampled from `y2`.

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"p-value", ~"rank diff",
    "wilcox.test", 0.7907, NA,
    "lm (ranks)", 0.7896, 1.56
    ),
  caption = "Mann-Whitney U and equivalent linear model",
  booktabs = TRUE
)
```


__Digression - Mann-Whitney U test and medians:__

It's worth noting that, with additional assumptions, the Man-Whitney U test is also a test for different _medians_.

This requires the assumption that the two samples have an equal shape, and therefore an equal variance. [Laerd](https://statistics.laerd.com/premium-sample/mwut/mann-whitney-test-in-spss-2.php) illustrates this concept using the diagram below: on the left, the two samples (men and women) have the same shape, so the Mann-Whitney U test tests for differences in the median score for men and women. On the right, the samples have different shapes, so it is a more general test for whether a randomly-selected woman's score is higher than a randomly-selected man's score (without telling us anything about medians).

```{r, echo = FALSE, fig.cap = "Man-Whitney U test and sample shapes", out.width = "95%", fig.align = "center"}
knitr::include_graphics("images/mann_whitney_laerd.png")
```

<!--chapter:end:05-two_means.Rmd-->

# Three or more means


## One-way ANOVA

The one-way analysis of variance (ANOVA) is used to determine whether there are any statistically significant differences between the means of three or more independent groups.

Examples include:

* Is there a difference in academic outcomes for pupils from ten different schools?
* Is there a difference daily coffee consumption between people in three different countries?

Intuitively, ANOVA is based on comparing the variance (or variation) between the groups, to variation
within each particular group. If the 'between' variation is much larger than the 'within' variation,
we are more likely to conclude that the means of the different groups are not equal. If the 'between' and 'within' variations are more similar in size, then we are less likely to conclude that there is a significant difference between sample means.

Why can't we just compare the means of every possible pair of groups, and see if any differences are statistically significant? The reason is that as the number of groups increases, the more likely we are to see differences that are due to chance alone. This means we are more likely to commit a Type I error, rejecting the null hypothesis (that there is no difference between the means) when the null hypothesis is in fact true.^[For example, if there were only two groups, we would be carrying out one comparison: the mean of Group A vs the mean of Group B. At a 0.05 level of significance, there would be a 5% chance of a Type I error. If we had three groups, there would be three comparisons (Group A vs Group B, Group A vs Group C, and Group B vs Group C), and we would have a 14.3% (1-0.95^3) chance of a Type I error.] An ANOVA controls for this additional risk of Type I errors, maintaining the overall or experimentwise error rate, which is typically $\alpha$ = 0.05.

It is important to note that the one-way ANOVA is an omnibus test statistic and cannot tell you _which_ specific groups were statistically significantly different from each other, only that at least two groups were different. To determine which specific groups differed from each other, you would need to use a post hoc test.^[Post hoc tests attempt to control the experimentwise error rate (usually $\alpha$ = 0.05) in the same manner that the one-way ANOVA is used instead of multiple t-tests. [Laerd](https://statistics.laerd.com/statistical-guides/one-way-anova-statistical-guide-4.php) suggests using the Tukey test (where there is homogeneity of variances in your samples) or the Games Howell test (where there is not).]

__Dataset:__

To illustrate, we will create a new dataset (`mydata_anova1`). We assume three groups (A, B and C) of normally-distributed variables, with means of 0, 1 and 0.5 respectively. We also create dummy variables for groups B and C (group A is our reference group, and so does not require an indicator):

```{r}
# Create dataset 'mydata_anova1' which is three groups:
set.seed(40)                      # Makes the randomised figures reproducible
n <- 20                           # Sample size of 20 for each group
mydata_anova1 <- data.frame(
                    value = c(rnorm_fixed(n, mu = 0, sd = 1),     # Group A
                              rnorm_fixed(n, mu = 1, sd = 1),    # Group B
                              rnorm_fixed(n, mu = 0.5, sd = 1)), # Group C
                    group = rep(c("a", "b", "c"), each = N)
                    ) %>%
  # Explicitly add indicator/dummy variables
  mutate(group_b = if_else(group == "b", 1, 0)) %>%             # Group B dummy
  mutate(group_c = if_else(group == "c", 1, 0))                 # Group C dummy

```

Here’s a sample of six rows from our new dataset, which includes the dummy variables:

```{r, echo = FALSE}
knitr::kable(
  (mydata_anova1[c(1, 2, 21, 22, 41, 42), ] %>% arrange(group)),
  caption = "Some randomly selected rows from our dataset",
  booktabs = TRUE)
```

Note that the data used in the remainder of this book varies from that used by Lindeløv in the original version, but the principles being discussed are exactly the same.

__ANOVA function:__

R has a package for ANOVA, in this case `car::Anova(aov())`. However, this is simply a 'wrapper' around the equivalent linear model, described below, and yields identical results.

__Equivalent linear model:__

The linear model assumes that the dependent variable can be predicted with a single mean for each group:

$y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + ... \qquad  H_0: y = \beta_0$

This assumption is illustrated below, in which there are assumed to be three groups (A, B and C). This extends the case with two groups, as was illustrated in the previous chapter, to cases with three or more groups. In this case, members of group B are identified by the dummy variable $x_1$, with the coefficient (or slope) of $\beta_1$, and members of group C are identified by the variable $x_2$, with the slope of $\beta_2$.


```{r dummy3, echo = FALSE, fig.cap = "Linear model equivalent to ANOVA with three groups", out.width = "75%", fig.align = "center"}
knitr::include_graphics("images/image_dummy2.png")
```

The null hypothesis of the linear model is that $\beta_1$ and $\beta_2$ are both zero; or equivalently, that all groups have the same mean of $\beta_0$. To test this hypothesis, an [F-test](https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/f-statistic-value-test/) is used. The F statistic in a regression is the result of a test where the null hypothesis is that all of the regression coefficients are equal to zero. The F-test compares your full model to one with no predictor variables (the intercept only model), and decides whether your added variables improved the model. If you get a significant result, then whatever coefficients you included in your full model improved the model’s fit (beyond what could be expected by chance alone).

In R, an F-test is carried out every time you run a linear regression, i.e. you do not have to specify it as an additional test.

__Comparison:__

The following code compares the ANOVA test in R with the identical linear model using dummy variables:

```{r, eval = FALSE}
# Anova
car::Anova(aov(value ~ group, data = mydata_anova1))

# Linear model
lm <- lm(value ~ 1 + group_b + group_c, data = mydata_anova1)
  lm %>% summary() %>% print(digits = 8) # show summary output

```

Here are the results, which are identical:

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"df", ~"df.residual", ~F.statistic, ~p.value,
    "Anova", 2, 57, 5, 0.00998,
    "lm", 2, 57, 5, 0.00998
    ),
  caption = "One-way ANOVA and equivalent linear model",
  booktabs = TRUE
)
```

Here we would reject the null hypothesis that there was no differences between the means of any of our groups at the 0.05 level of significance (because p = 0.00998).

It should be emphasised that the results of the ANOVA and the linear model are identical _by construction_, as they are both an F-test that compares the full model (with group dummies) to a model with an intercept only.

### Kruskal-Wallis

The non-parametric version of the ANOVA is the Kruskal-Wallis test. We would need to use this test if our dependent variable was ordinal rather than continuous. We would also use the non-parametric version if other assumptions of the one-way ANOVA did not hold, including (1) that the dependent variable was approximately normally distributed for each category of the independent variable, and (2) homogeneity of variances.

__Equivalent linear model:__

The Kruskal-Wallis is essentially a one-way ANOVA test on __ranks__. It can be expressed as the following linear model:


$rank(y) = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + ... \qquad  H_0: y = \beta_0$


__Comparison:__

Here is a comparison of the Kruskal-Wallis test and the equivalent linear model (the equivalent ANOVA test is also included for completeness, which we've seen is just a 'wrap' around the linear model).

```{r, eval = FALSE}
# Kruskal-Wallis
kruskal.test(value ~ group, data = mydata_anova1)

# Linear model on ranks
lm <- lm(rank(value) ~ 1 + group_b + group_c, data = mydata_anova1)
  lm %>% summary() %>% print(digits = 8) # show summary output

# Anova on ranks (which is a wrapper around the linear model above)
car::Anova(aov(rank(value) ~ group, data = mydata_anova1))

```

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~"df", ~p.value,
    "Kruskal", 2, 0.0203,
    "lm", 2, 0.0177
    ),
  caption = "Kruskal-Wallis test and equivalent linear model",
  booktabs = TRUE
)
```

The p-value of the two tests are similar, though not identical. In this example, we would reject the null hypothesis that all groups had equal means (at the 0.05 level of significance).


## Two-way ANOVA

The two-way ANOVA compares the means of groups that have been split on two independent variables, or 'factors'.

For example: is there an interaction between gender and educational level on test anxiety among university students? Here gender (males / females) and education level (high school / undergraduate / postgraduate) are your independent variables or factors.

A two-way ANOVA tests three hypotheses:

* That the population means of the first factor (e.g. each gender) are equal;
* That the population means of the second factor (e.g. each education level) are equal; and
* That there is no interaction between the two factors - i.e. that the relationship between anxiety and gender does not depend on education level, or that the relationship between anxiety and education does not depend on gender.

The first two hypotheses relate to the relationship between each factor and the dependent variable, referred to as 'main effects'. Each of these is like a one-way ANOVA, but in the context of a larger model. The third hypothesis relates to the 'interaction effect'. Here we will focus on the interaction effect.

__Updated dataset:__

To show the modelling in R, we'll add another factor to our example dataset, `mood`, which reports whether a person is happy or sad. We also use this to create a dummy variable, `mood_happy`, which takes the value of 1 if the person is happy or 0 if they are sad.

```{r}
mydata_anova2 <- mydata_anova1 %>%
  # 60 observations in total
  mutate(mood = rep(c("happy", "sad"), 30)) %>% 
  # The dummy variable
  mutate(mood_happy = if_else(mood == "happy", 1, 0))
```

Here's a selection of six rows from the updated dataset:
```{r, echo = FALSE}
knitr::kable(
  (mydata_anova2[c(1, 2, 21, 22, 41, 42), ] %>%
    arrange(group)),
  caption = "Some randomly selected rows from our dataset",
  booktabs = TRUE)
```

__Equivalent linear model:__

The two-way ANOVA can test for the interaction between two factors (let's ignore the main effects for now). It is equivalent to the following linear model, which is now expressed using matrix notation:

::: {.center data-latex=""}

$y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_2  X_3 \qquad  H_0: \beta_3 = 0$

:::

Here $X_1$ and $X_2$ represent the two factors in our model (in this example, the 'group' and 'mood' of each observation, respectively). Each $\beta_i$ is a vector of values that relates to the levels within each factor. In our example, $\beta_1$ will have two values that correspond to the group dummy variables (`group_b` and `group_c`) and $\beta_2$ will be single value corresponding to the dummy variable for mood (`mood_happy`). The intercept $\beta_0$, to which all other $\beta$s are relative, is now the mean for the first level of all factors (people in group A who are sad).

$\beta_3$ is a vector that relates to the interactions of the factors. Here, it will be a vector comprising two values, corresponding to the combinations of the group and mood dummy variables (`group_b` and `mood_happy`, and `group_c` and `mood_happy`). The null hypothesis (of $\beta_3 = 0$) is that all values in this vector are zero, and that there is no interaction between group and mood when explaining the dependent variable, `value`.

To test this null hypothesis, we are going to carry out an F-test of two nested models:

* a full model, which includes both factors and the interaction terms; and
* a restricted or null model, which includes both factors but no interaction term.

This is similar to the F-test used in the one-way ANOVA above, but in this case our null model includes the two factors and not just an intercept term.

__Comparison:__

The code for running the ANOVA, using the package in R, is as follows:

```{r, eval = FALSE}
# Two-way ANOVA, built-in function
car::Anova(aov(value ~ mood + group + mood:group, data = mydata_anova2))
```

The equivalent linear model, which includes an interaction between the two group dummy variables and the mood dummy variable, is specified as follows:

```{r, eval = FALSE}
lm(value ~ 1 + group_b + group_c + mood_happy +
           group_b:mood_happy + group_c:mood_happy,
        data = mydata_anova2)
```

The results of the above model will give us the p-values of _all_ the interaction terms (in this case, two) and tell us if any of these are statistically significant. But recall that our null hypothesis is that _none_ of the interaction terms are significant, and we can't rely on individual tests for this because of the increased risk of errors (as explained in the  previous section on one-way ANOVAs). This is why we use the two-way ANOVA, which is an F-test that compares the full and null models:

```{r, eval = FALSE}
# null model, without interactions
null <- lm(value ~ 1 + group_b + group_c + mood_happy, data = mydata_anova2)

# full model, with interactions
full <- lm(value ~ 1 + group_b + group_c + mood_happy + group_b:mood_happy +
             group_c:mood_happy, data = mydata_anova2)

# ANOVA using the two models above.
anova(null, full, test = "F")    # anova() uses an F test by default,
                                # but here it's made explicit
```

The results of the two approaches are presented in the table below. This shows that the two approaches are identical F-tests with the same resulting p-value.

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Test, ~df, ~df.res, ~F.value, ~p.value,
    "ANOVA", 2, 54, 0.2977, 0.7437,
    "lm", 2, 54, 0.2977, 0.7437
    ),
  caption = "Two-way ANOVA and equivalent linear model",
  booktabs = TRUE
)
```

On this basis of the test above, we would fail to reject the null hypothesis that there was no interaction between the two factors in our model (group and mood), at the 0.05 level of significance.

## ANCOVA

This adds a _continuous_ independent variable, or covariate, to the model (e.g. age), in addition to one or more categorical independent variables (e.g. gender or education level).

An analysis of covariance (ANCOVA) evaluates whether the mean of the dependent variable is equal across levels of a categorical independent variable, while statistically controlling for the effects of other continuous variables (e.g. age) that are not of primary interest, known as covariates.

__Updated dataset:__

Here will will add a covariate to our one-way ANOVA above. In addition to the `group` dummy variables, we update our data set to include each subject's `age`, which we assume is correlated with the dependent variable, `value`:

```{r}
# create a new column with the continuous variable 'age'
mydata_anova3 <- mydata_anova1 %>%
  mutate(age = value + rnorm_fixed(nrow(.), sd = 3))
```

Here's a selection of six rows from the updated dataset:

```{r, echo = FALSE}
knitr::kable(
  (mydata_anova3[c(1, 2, 21, 22, 41, 42), ] %>% arrange(group)),
  caption = "Some randomly selected rows from our dataset",
  booktabs = TRUE)
```

__ANOVA function:__

An ANCOVA can be carried out using the `Anova()` function and including the covariate (in this case `age`) as an independent variable.

```{r, eval = FALSE}
car::Anova(aov(value ~ group + age, mydata_anova3))
```

__Equivalent linear model:__

The same results can be achieved by using F-tests to compare two sets of linear models: (i) the full model and the nested model which excludes `age`, and (ii) the full model and the nested model that excludes the `group` dummy variables. Again, the F-tests are carried out using the `anova()` function, which uses an F-test by default.

The full model can be formulated as follows:

::: {.center data-latex=""}

$y = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + ... + \beta_3 \cdot age$

:::

where the value of $y$ varies by group, as represented here by the dummy variables $x_1$ and $x_2$, and also by $age$.

This can be illustrated below. The ANCOVA tests whether there is difference in the mean `y` for the three groups, after controlling for age (the vertical shift shown by $\beta_1$ and $\beta_2$). It also tests whether the slope $(\beta_3)$ is statistically significant.

```{r, echo = FALSE, fig.cap = "Linear model equivalent to ANCOVA with three groups", out.width = "75%", fig.align = "center"}
knitr::include_graphics("images/image_ancova.png")
```

Here we run the linear model in R. The resulting p-values relate to the null hypotheses that `age` and `group` (respectively) have no effect on the dependent variable, in this case `value`.

```{r, eval = FALSE}
# full model, with group and age variables
full <- lm(value ~ 1 + group_b + group_c + age, mydata_anova3)
# model without age
null_age <- lm(value ~ 1 + group_b + group_c, mydata_anova3)
# model without groups
null_group <- lm(value ~ 1 + age, mydata_anova3)
# result for age
anova(null_age, full)
# results of group
anova(null_group, full)
```

__Comparison:__

The results of the two approaches are presented in the table below:

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~Term, ~Model, ~df, ~res.df, ~F.value, ~p.value,
    "age", "Anova", 1, NA, 5.2002, 0.02641,
    "age", "lm", 1, 56, 5.2002, 0.02641,
    "group", "Anova", 2, NA, 4.6929, 0.01305,
    "group", "lm", 2, 56, 4.6929, 0.01305
    ),
  caption = "ANCOVA and linear model",
  booktabs = TRUE
)
```

Based on these results, we would reject the null hypothesis that there was no relationship between `value` and `group`, even after controlling for differences in `age`. We would also reject the null hypothesis that `value` was not related to `age`.

<!--chapter:end:06-three_plus_means.Rmd-->

# Proportions and chi squared


This chapter looks at methods used for analyzing relationships in _categorical_ data. The variable of interest is not a single continuous variable (e.g. how income or weight varies between groups) but the relative _count_ or _proportion_ of observations that fall into each category.

A key point is that the chi-squared test used in these cases is equivanet to a test of nested Poisson regression models.

## Goodness of fit

A test of __goodness-of-fit__ establishes whether an observed frequency distribution differs from a theoretical distribution. For example, we could test the hypothesis that a random sample with 44 men and 56 women has been drawn from a population in which men and women are equal in frequency, i.e. with the theoretical distribution of 50 men and 50 women.

__Sample dataset:__

Assume we have one category (mood) with three possible levels ('happy', 'sad' or 'meh'). We have a sample of 120 observations in total, as generated by the code below. We also add dummy variables to represent 'happy' and 'sad' for use in the linear model.

```{r}
mydata_chi1 <- data.frame(mood = c("happy", "sad", "meh"),
               counts = c(60, 90, 70)) %>%

  # Dummy variables to be used in linear model
  mutate(mood_happy = if_else(mood == "happy", 1, 0)) %>%
  mutate(mood_sad = if_else(mood == "sad", 1, 0))
```

```{r, echo = FALSE}
knitr::kable(
  (mydata_chi1),
  caption = "Categorical data with one variable",
  booktabs = TRUE)
```

In this example, we may want to test whether the proportions of people with each mood in the underlying population are equal, based on our observation of 120 people.

__Chi-squared test:__

The goodness-of-fit test is based on the following test statistic:

::: {.center data-latex=""}

$\chi^2 = \sum \frac{(O_i - Ei)^2}{E_i}$

:::

where $O_i$ is the observed count in each category and $E_i$ is the expected count in each category.
The test statistic follows a chi-squared ($\chi^2$) distribution where the degrees of freedom are equal to the number of categories minus one, i.e. $d.f. = rows - 1$ (in this example, df = 2).

A worked example of a goodness-of-fit test is provided [in this video](https://www.khanacademy.org/math/statistics-probability/inference-categorical-data-chi-square-tests/chi-square-goodness-of-fit-tests/v/chi-square-statistic) by Khan Academy.

R's built-in chi-squared test, `chisq.test`, compares the proportion of counts in each category with the expected proportions. By default, the expected proportions in each category are assumed to be equal.

```{r}
#Built-in test
chisq.test(mydata_chi1$counts)
```

In this example, we would reject the null hypothesis that the proportion of people with each mood are all equal (p = 0.04151).

__Equivalent linear model:__

The equivalent linear model is a [Poisson regression](https://en.wikipedia.org/wiki/Poisson_regression). This is a type of Generalized Linear Model (GLM). Poisson regressions use the natural log of the dependent variable, and so are sometimes referred to as a _log-linear models_. The follow model specification can be used:

::: {.center data-latex=""}

$log(y) = \beta_0 + \beta_1 \cdot x_1 + \beta_2 \cdot x_2 + ...$

:::

where in this example $x_1$ and $x_2$ would be dummy variables representing 'happy' and 'sad' moods.

There are two reasons why we need to use a log-linear model rather than the OLS linear regression model, as explained in the book ['Broadening Your Statistical Horizons'](https://bookdown.org/roback/bookdown-bysh/) by Julie Legler and Paul Roback:

1. Since a Poisson random variable is a count, its minimum value is zero and, in theory, the maximum is unbounded. A line from an OLS is model certain to yield negative values for certain values of the dependent variable, $x$; however, $y$ can only take values from 0 to $\infty$.

2. The equal variance assumption in linear regression inference is violated, because as the expected value of a Poisson variable increases so too does the variance. With a Poisson distribution, the variance is equal to the expected value of the dependent variable, that is $E(Y) = VAR(Y)$.

These issues are addressed by taking the natural log of the dependent variable. It is assumed that the resulting logarithm of the dependent variable can be modeled by a linear combination of the independent variable(s) used in the model.

Because we are using the natural log of the dependent variable, the coefficients from our regression, $\beta_1$ and $\beta_1$, are interpreted as the _percentage_ differences in the number of people whose moods are happy or sad, relative to the reference level of 'meh'.^[Technically, the percentage difference is equal to $e^{\beta_1}$ and $e^{\beta_2}$, where $\beta_1$ and $\beta_2$ are continuously compounded rates of growth between 0 and 1. So for example, in the example below, the coefficient on 'sad' is $\beta_2 = 0.2513$. This mean the proportion of people in the 'sad' category are higher by $e^{0.2513} - 1 = 0.286 = 28.6\%$ than the proportion of people in the 'meh' category. This is equal to difference of the count of people in these two categories i.e. 90 'sad' people and 70 'meh' people.] Because of this, it can be used as a test for differences of proportions; that is, whether there is a significant difference in the percentage of people in each category.

To assess whether the differences between categories (moods) are statistically significant we compare two nested models, just as we did with the ANOVA and ANCOVA tests in the previous chapter. In this case we compare a 'full' Poisson regression model, which includes our dummy variables, and a 'null' model which does not. Instead of using the F-test to compare nested models (as was the case with ANOVA/ANCOVA), here we use a ['score test'](https://en.wikipedia.org/wiki/Score_test), specifically the Rao test. This gives us a test statistic that follows a $\chi^2$ distribution, with the degrees of freedom being equal to the additional parameters in the full model compared to the null model.

The code is as follows:

```{r}
full <- glm(counts ~ 1 + mood_happy + mood_sad,
            data = mydata_chi1,
            family = poisson())
null <- glm(counts ~ 1,  data = mydata_chi1, family = poisson())
anova(null, full, test = "Rao")

```

__Comparison:__

As seen above, the p-value is identical under both the built-in chi-squared test and the equivalent linear model:

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~model, ~"Chi-squared", ~"df", ~p.value,
    "chisq.test", 6.3636, 2, 0.04151,
    "glm", 6.3636, 2, 0.04151
    ),
  caption = "Goodness-of-fit: chi-squared test and equivalent linear model",
  booktabs = TRUE
)
```

## Contingency tables

Contingency tables are used in statistics to summarize the relationship between two categorical variables. They are used to test whether the frequency distribution differs between two or more groups. This can be tested using R"s built-in chi-squared test (`chisq.test`), similar to the goodness-of-fit test above, or again as two nested linear models (Poisson regressions), one of which includes dummy variables representing the interaction between the two categories in our table.

__Sample dataset:__

As an example, we might like to test whether a subjects mood (happy, sad or meh) is related to sex (male or female). We can create the following data set:

```{r}
mydata_chi2 <- data.frame(
  sex = c("male", "female"),
  happy = c(70, 100),
  meh = c(32, 30),
  sad = c(120, 110)
)
```

```{r, echo = FALSE}
knitr::kable(
  (mydata_chi2),
  caption = "A contingency table",
  booktabs = TRUE)
```


__Chi-squared test:__

As shown above, a contingency table is a table that lists the frequencies of occurrence for categories of __two__ variables. The first variable is shown in rows, and the second variable is shown in columns.

Contingency tables can be used to assess whether the proportion of observations in one category depends on, or is _contingent_ upon, the other category in the table. There are actually two types of tests:

* A test of __homogeneity__. This tests the null hypothesis that _different populations_ have the same proportions of some characteristics. The key difference from the test of independence is that there are multiple populations that the data is drawn from. The null hypothesis is that the proportion of X is the same in all populations studied.
* A test of __independence__. A test of independence tests the null hypothesis that there is no association between the two variables in a contingency table where the data is all drawn from _one population_. The null hypothesis is that X and Y are independent.

Both these tests involve the exactly the same mathematical procedures and only differ only in terms of the hypothesis being tested. Some further reading can be found [here](http://www.u.arizona.edu/~kuchi/Courses/MAT167/Files/LH_LEC.0640.HypTest.IndepHomog.pdf).

These tests are based on a similar test statistic to the goodness-of-fit test:

::: {.center data-latex=""}

$\chi^2 = \sum \frac{(O_i - Ei)^2}{E_i}$

:::

where $O_i$ is the observed count in each category and $E_i$ is the expected count in each category.
The test statistic follows a chi-squared ($\chi^2$) distribution where the degrees of freedom are found by $d.f. = (rows - 1)(columns - 1 )$.

A worked example of a chi-squared test is provided [in this video](https://www.khanacademy.org/math/statistics-probability/inference-categorical-data-chi-square-tests/chi-square-tests-for-homogeneity-and-association-independence/v/chi-square-test-homogeneity) by Khan Academy.

R’s built-in chi squared test, `chisq.test`, can be used to assess whether the distribution of observations in one category is contingent on the distribution of observations under the other category. Note that first we must convert our data to a matrix and drop the first column (in this case  `sex`):

```{r}
# Convert data to matrix format, need for the built-in chi-squared
mydata_chi2_matrix <- mydata_chi2 %>%
  select(-sex) %>%
  as.matrix()
```

Now carry out the chi-squared test:
```{r}
# Built-in chi-squared
chisq.test(mydata_chi2_matrix)
```

Based on this p-value we would not reject the null hypothesis that sex and mood were independent at the 0.05 level of significance.

__Equivalent linear model:__

The equivalent linear model is a Poisson regression with interaction terms between the two sets of dummy variables (in this case one set of dummy variables is for mood, the other is for sex).

Here we are testing for the interaction between two the two categories, just as we did with the two-way ANOVA (though here our dependent variable is the natural log of the count of observations, rather than the value of a single continuous variable). The test is equivalent to the following linear model, which is expressed using matrix notation:

::: {.center data-latex=""}

$log(y) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_2  X_3 \qquad  H_0: \beta_3 = 0$

:::

Here $X_1$ and $X_2$ represent the two categories in our model (in this example, mood and sex). $\beta_3$ is a vector that relates to the interactions of the categories. Here, it will be a vector comprising two values, corresponding to the combinations of the sex and mood dummy variables (`male` and `mood_happy`, and `male` and `mood__meh`). The null hypothesis (of $\beta_3 = 0$) is that all values in this vector are zero, and that there is no interaction between sex and mood when explaining the distribution of observations in each category.

We begin by expressing our data in 'long' format, including dummy variables to identify the groups of people who are happy and 'meh'. This following data will be used in our linear model:

```{r, echo = FALSE}
mydata_chi2_long <- mydata_chi2 %>%
  gather(mood, Freq, happy:sad) %>%
  # Create dummy variables for mood and sex
  mutate(mood_happy = if_else(mood == "happy", 1, 0)) %>%
  mutate(mood_meh = if_else(mood == "meh", 1, 0)) %>%
  mutate(sex_male = if_else(sex == "male", 1, 0))
```

```{r, echo = FALSE}
knitr::kable(
  (mydata_chi2_long),
  caption = "Contingency table data in long format with dummy variables",
  booktabs = TRUE)
```

The test involves a comparison of two nested linear models: a full model which includes the interaction terms between the two sets of dummy variables, and the null model which excludes the interaction terms. Again we use the (Rao) score test.

```{r}
full <- glm(Freq ~ 1 + mood_happy + mood_meh + sex_male +
                   mood_happy*sex_male + mood_meh*sex_male,
            data = mydata_chi2_long, family = poisson())
null <- glm(Freq ~ 1 + mood_happy + mood_meh + sex_male,
            data = mydata_chi2_long, family = poisson())
anova(null, full, test = "Rao")
```

Note that the only difference between the nested models is the two interaction terms. Intuitively, we are testing whether the _interaction_ of the two categories is statistically significant, i.e. whether the interaction between mood and sex can explain the distribution of observations over and above the individual effects of mood and sex alone.

__Comparison:__

As summarized in the table below, the linear model gives the same test statistic as the built-in chi-squared test, has the same degrees of freedom (which is equal to the number of interaction terms), and the same p-value.

```{r, echo = FALSE}
knitr::kable(
  tribble(
    ~model, ~"Chi-squared", ~"df", ~p.value,
    "chisq.test", 5.0999, 2, 0.07809,
    "glm", 5.0999, 2, 0.07809
    ),
  caption = "Contingency table: chi-squared test and linear model",
  booktabs = TRUE
)
```

<!--chapter:end:07-proportions.Rmd-->


# Appendix - Types of variables {#appendixtypes}

Variables (or measurements) fall into two categories: __discrete__ or __continuous__.

## Discrete variables

Discrete variables are also known as categorical variables. They are descriptions of categories into which observations can fall. Discrete variables can be further categorized as either _nominal_ or _ordinal_.

__Nominals__ variables have two or more categories which do not have an intrinsic order. In other words, there is no basis for ranking the categories. Examples of nominal variables include:

* Whether a person has a landline telephone could be categorized as "yes" or "no" (two categories)
* The US state in which a person lives (50 categories)

__Ordinal__ variables have two or more catogories which do have an intrinsic order - that is, they _can_ be ordered or ranked. Examples include:

* Levels of agreement, e.g. asking a survey respondent if they (i) strongly agree, (ii) agree, (iii) neither agree nor disagree, (iv) disagree or (v) strongly disagree with a question.
* Educational attainment could be recorded in a survey using four categories: (i) no high school degree, (ii) high school degree, (iii) college degree, or (iv) postgraduate degree. Here the categories can be ranked based on the level of educational attainment.

For ordinal variables, the interval or distance between the categories does not have a meaningful interpretation. For example, we cannot say that the distance between (i) no highschool degree and (ii) high school degree is the same as the distance bewteen (iii) college degree and (iv) postgraduate degree.

## Continuous variables

Continuous variables are numbers rather than categories. Continuous variables can be further categorized as either _interval_ or _ratio_ variables.

__Interval__ variables have a numeric value and can be measured along a continuum. The difference between values is interpretable. An example is temperature measure in Fahrenheit: the difference between 20F and 30F is the same as the difference bewtween 30F to 40F. However Fahrenheit is not a ratio variables. For example, 40 degrees is not "twice as hot" as 20 degrees.

__Ratio__ variables are interval variables for which you can construct a meaningful fraction. Examples include height, weight, distance and income. For example, you could say that an income of \$40,000 was twice as much as an income of \$20,000. "Count" variables are also ratio variables; for example, the number of survey respondents who would vote for a presidential candidate. A condition of ratio variables is that 0 (zero) of the measurement indicates that there is none of that variable (e.g. $0 indicates zero income). This was not the case of temperature measured in Farenheit, as 0F does not mean there is "no temperature".

The four types of variables above form a hierarchy, where ratio variables are the highest:

Nominal < Ordinal < Interval < Ratio

At each level up the hierarchy, the current level includes all of the qualities of the one below it and adds something new.

<!--chapter:end:08_appendixA.Rmd-->

